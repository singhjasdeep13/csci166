{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhjasdeep13/csci166/blob/main/CSCI_166_FrozenLake_DirectEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen Lake w/ Value Iteration & Direct Evaluation"
      ],
      "metadata": {
        "id": "rTwoHidK7QXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTUHNC0Oien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKf_jjk9OgN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc72e94b-f7c2-48d5-cf83-4d939194e167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_q5-OvYOldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "c551dc15-e262-4b87-8756-5eb5a8c47142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration"
      ],
      "metadata": {
        "id": "lompMKJo2O2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000, theta=0.0001):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            action_values = []  # Store Q values for all actions in this state\n",
        "\n",
        "            for a in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q_value += prob * (reward + gamma * V_prev[next_state])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            # Update V[s] with the max Q value\n",
        "            V[s] = max(action_values)\n",
        "            # Update policy[s] with the action that maximizes Q value\n",
        "            policy[s] = np.argmax(action_values)\n",
        "\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta and 0:\n",
        "            break\n",
        "        V_prev = V.copy()\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Apply student's Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")\n",
        "\n",
        "# Evaluate student's solution (Optional)\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ =  env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kv_pwvY4YR",
        "outputId": "1146cb5f-b1c6-4273-c815-fd560d1b37e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " optimal_V=\n",
            "[[0.07 0.06 0.07 0.06]\n",
            " [0.09 0.   0.11 0.  ]\n",
            " [0.15 0.25 0.3  0.  ]\n",
            " [0.   0.38 0.64 0.  ]]\n",
            "Average Reward: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Extraction & Policy Evaluation"
      ],
      "metadata": {
        "id": "9Xc8WhLW2Spz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(env, V, gamma=0.9):\n",
        "  policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "  for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      action_values = []  # Store Q values for all actions in this state\n",
        "\n",
        "      for a in range(env.action_space.n):\n",
        "          q_value = 0\n",
        "          for prob, next_state, reward, done in env.P[s][a]:\n",
        "              q_value += prob * (reward + gamma * V[next_state])\n",
        "          action_values.append(q_value)\n",
        "\n",
        "      # Update policy[s] with the action that maximizes Q value\n",
        "      policy[s] = np.argmax(action_values)\n",
        "  return policy\n",
        "\n",
        "def policy_evaluation(env, policy, gamma=0.9, num_iterations=1000, theta=0.0001):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    for _ in range(num_iterations):\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            a = policy[s]\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q_value += prob * (reward + gamma * V_prev[next_state])\n",
        "            # Update V[s] with the max Q value\n",
        "            V[s] = q_value\n",
        "\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta and 0:\n",
        "            break\n",
        "        V_prev = V.copy()\n",
        "    return V"
      ],
      "metadata": {
        "id": "XFUHBMc1kiOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_policy_evalution = policy_evaluation(env, optimal_policy)\n",
        "print (f\"V_policy_evalution=\\n{V_policy_evalution}\")\n",
        "V_policy = policy_extraction(env, V_policy_evalution)\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n V_policy=\\n{V_policy.reshape((4,4))}\")"
      ],
      "metadata": {
        "id": "_ZxtAYnEmeIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8072a3c3-3ae8-44fe-caf4-c3bf6077c668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_policy_evalution=\n",
            "[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n",
            " 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n",
            " 0.         0.3799359  0.63902015 0.        ]\n",
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " V_policy=\n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "4-nohV0on3fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset(seed=42)\n",
        "init_policy = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "print (f\"optimal policy= \\n{init_policy.reshape((4,4))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9uKUHB9o2dr",
        "outputId": "1f632789-bcf8-4e5c-8f20-3545141c7cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateEpisodes(env, policy, num_episodes=5):\n",
        "    total_reward = 0\n",
        "    episodes = []\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode = []\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            newstate, reward, done, _ = env.step(action)\n",
        "            episode.append((int(state), action, newstate, reward))\n",
        "            state = newstate\n",
        "        episodes.append(episode)\n",
        "    return episodes\n",
        "\n",
        "training_episodes = GenerateEpisodes(env, init_policy)\n",
        "for i, e in enumerate(training_episodes):\n",
        "  print (f\"training_episode=({i=}):\\n{e}\")\n",
        "\n",
        "training_episodes2 = GenerateEpisodes(env, init_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htwNCLIJpo16",
        "outputId": "5b2e503c-4afa-4f66-9cdc-faea69d20c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_episode=(i=0):\n",
            "[(0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=1):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=2):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=3):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 2, 0.0), (2, 0, 1, 0.0), (1, 3, 1, 0.0), (1, 3, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0)]\n",
            "training_episode=(i=4):\n",
            "[(0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Evaluation\n",
        "\n",
        "## Evaluate Single Episode"
      ],
      "metadata": {
        "id": "f_yt-sIhrGwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EvaluateEpisode(env, e, V_DE, V_Counts, gamma=0.9):\n",
        "    future_reward = 0\n",
        "    for t in reversed(e):  # Iterate in reverse order\n",
        "        future_reward = t[3] + gamma * future_reward\n",
        "        V_DE[t[0]] = future_reward+V_DE[t[0]]\n",
        "        V_Counts[t[0]] = V_Counts[t[0]]+1\n",
        "    return V_DE, V_Counts"
      ],
      "metadata": {
        "id": "jB40XhParIyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Episode 1\n",
        "\n"
      ],
      "metadata": {
        "id": "NRQJmwWJ3E0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_DE = np.zeros((env.observation_space.n))\n",
        "V_Counts = np.zeros((env.observation_space.n))\n",
        "V_DE, V_Count = EvaluateEpisode(env, training_episodes[0], V_DE, V_Counts, 0.9)\n",
        "V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n",
        "print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n",
        "print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n",
        "print (f\"V=\\n{V.reshape((4,4))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaUV7MWJ29oe",
        "outputId": "a30fd42b-6c3c-4197-f82e-fc06c8ebfa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_DE=\n",
            "[[0.59049 0.      0.      0.     ]\n",
            " [0.6561  0.      0.      0.     ]\n",
            " [0.729   0.81    0.9     0.     ]\n",
            " [0.      0.      1.      0.     ]]\n",
            "V_Counts=\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 1. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n",
            "V=\n",
            "[[0.59049 0.      0.      0.     ]\n",
            " [0.6561  0.      0.      0.     ]\n",
            " [0.729   0.81    0.9     0.     ]\n",
            " [0.      0.      1.      0.     ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-000a7187090b>:4: RuntimeWarning: invalid value encountered in divide\n",
            "  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate All Episodes"
      ],
      "metadata": {
        "id": "wlbXCln-4EXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_DE = np.zeros((env.observation_space.n))\n",
        "V_Counts = np.zeros((env.observation_space.n))\n",
        "for e in training_episodes2:\n",
        "    V_DE, V_Count = EvaluateEpisode(env, e, V_DE, V_Counts, 0.9)\n",
        "V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n",
        "print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n",
        "print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n",
        "print (f\"V_DirectEvaluation=\\n{np.round(V.reshape((4,4)),2)}\")\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OswEPertn95",
        "outputId": "866fd35a-c920-4b90-9a97-5c29f386d3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_DE=\n",
            "[[ 879.44773993   12.02033939   32.45370905    0.        ]\n",
            " [ 910.67996731    0.           63.889207      0.        ]\n",
            " [ 943.61931212  887.58499323  418.52594351    0.        ]\n",
            " [   0.         1065.13267976 1369.46029196    0.        ]]\n",
            "V_Counts=\n",
            "[[12819.   227.   503.     0.]\n",
            " [ 9779.     0.   646.     0.]\n",
            " [ 6632.  3600.  1432.     0.]\n",
            " [    0.  2876.  2159.     0.]]\n",
            "V_DirectEvaluation=\n",
            "[[0.07 0.05 0.06 0.  ]\n",
            " [0.09 0.   0.1  0.  ]\n",
            " [0.14 0.25 0.29 0.  ]\n",
            " [0.   0.37 0.63 0.  ]]\n",
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " optimal_V=\n",
            "[[0.07 0.06 0.07 0.06]\n",
            " [0.09 0.   0.11 0.  ]\n",
            " [0.15 0.25 0.3  0.  ]\n",
            " [0.   0.38 0.64 0.  ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-440a84a75783>:5: RuntimeWarning: invalid value encountered in divide\n",
            "  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning"
      ],
      "metadata": {
        "id": "RMo2qLGB0ZfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def q_learning(env, num_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    I used ChatGPT to help optimize these functions using the structure of the\n",
        "    value iteration function. The optimal policy and optimal Q values vary\n",
        "    based on the randomized choices meant to explore the entire environment\n",
        "    using an epsilon of 0.1\n",
        "\n",
        "    Implements the Q-Learning algorithm with Epsilon-Greedy action selection.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        num_episodes: Number of episodes to run.\n",
        "        alpha: Learning rate.\n",
        "        gamma: Discount factor.\n",
        "        epsilon: Exploration rate.\n",
        "\n",
        "    Returns:\n",
        "        Q: The learned Q-value function.\n",
        "        policy: The optimal policy derived from Q.\n",
        "    \"\"\"\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-Greedy randomized action selection\n",
        "            if random.randint(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            # Take the action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value using Bellman equation\n",
        "            best_next_action = np.argmax(Q[next_state])  # Max Q value for the next state\n",
        "            Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    return Q, policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CiwZlbyv3SG",
        "outputId": "e5b74bef-7f63-465a-aa77-184c6ed4c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning\n",
        "Q, optimal_policy = q_learning(env)\n",
        "print(f\"Optimal Policy: \\n{optimal_policy.reshape((4, 4))}\")\n",
        "print(f\"Optimal Q-Values:\\n{Q}\")\n",
        "\n",
        "# Optimal policy\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM0foN_Zy40t",
        "outputId": "928386c7-d374-40ca-e120-6eb1077e9a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: \n",
            "[[0 3 1 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            "Optimal Q-Values:\n",
            "[[0.05533788 0.04856475 0.0497748  0.0464122 ]\n",
            " [0.03718335 0.03493501 0.03559337 0.04825605]\n",
            " [0.05455405 0.06231444 0.04745184 0.04277469]\n",
            " [0.01282524 0.02223893 0.01260682 0.04094245]\n",
            " [0.08280808 0.06060981 0.06219694 0.04723442]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.09516035 0.03511808 0.07915709 0.01340697]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.03721352 0.08773549 0.07671951 0.12437365]\n",
            " [0.122975   0.18958664 0.15301933 0.07323143]\n",
            " [0.32737138 0.20945979 0.1414324  0.03572877]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.12654909 0.13457577 0.3986834  0.34558009]\n",
            " [0.28842412 0.63923528 0.40951795 0.47002767]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Average Reward: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the Q-Learning values to the Value Iteration values"
      ],
      "metadata": {
        "id": "RI2RBU281b68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Value Iteration values:\n",
        "\n",
        "optimal policy=\n",
        "[[0 3 0 3]\n",
        " [0 0 0 0]\n",
        " [3 1 0 0]\n",
        " [0 2 1 0]]\n",
        " optimal_V=\n",
        "[[0.07 0.06 0.07 0.06]\n",
        " [0.09 0.   0.11 0.  ]\n",
        " [0.15 0.25 0.3  0.  ]\n",
        " [0.   0.38 0.64 0.  ]]\n",
        "Average Reward: 0.69\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Q-Learning values:\n",
        "\n",
        "Optimal Policy:\n",
        "[[0 3 0 1]\n",
        " [0 0 1 0]\n",
        " [3 1 0 0]\n",
        " [0 2 1 0]]\n",
        "Optimal Q-Values:\n",
        "[[0.07633568 0.06776639 0.07294079 0.05973072]\n",
        " [0.03932479 0.0360577  0.04771194 0.05959271]\n",
        " [0.06961512 0.06619583 0.06782806 0.053756  ]\n",
        " [0.01055646 0.04466839 0.01655398 0.0241352 ]\n",
        " [0.11523872 0.06910472 0.05657164 0.06350721]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.08215437 0.12019106 0.11604543 0.02791263]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.0735724  0.07015024 0.10810174 0.15213926]\n",
        " [0.11471993 0.22846509 0.20324377 0.13547533]\n",
        " [0.26964691 0.24076213 0.15962786 0.08372177]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.17035553 0.20893492 0.28754611 0.26027103]\n",
        " [0.31257376 0.55692456 0.3983626  0.37721044]\n",
        " [0.         0.         0.         0.        ]]\n",
        "Average Reward: 0.64\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mp239QhU13FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Value Iteration and Q-Learning values we see a few differences. Q-Learning policies, values, and rewards all vary based on the random movements meant to explore the entire environment. Many times the rewards are less and the policy can be far from optimal. Sometimes it reaches close to the optimal values and policy provided through value iteration as seen above. Very rarely is the reward higher than value iteration even if the optimal policies match."
      ],
      "metadata": {
        "id": "YaNgIVUx2kNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning with decaying epsilon"
      ],
      "metadata": {
        "id": "hO0TKazR6luy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def q_learning2(env, num_episodes=1000, alpha=0.1, gamma=0.9, epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.5):\n",
        "    \"\"\"\n",
        "    I am still struggling to understand this concept and used ChatGPT to help\n",
        "    optimize this code. The only differences between this function and the\n",
        "    original above is the starting and ending values of epsilon that is\n",
        "    affected by decay rate, which in turn lowers the exploration rate of the\n",
        "    agent after each episode. I have tested with decay_rate values ranging\n",
        "    from 0.95 to 0.5 and the results vary each execution.\n",
        "\n",
        "    Implements the Q-Learning algorithm with Epsilon-Greedy action selection.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        num_episodes: Number of episodes to run.\n",
        "        alpha: Learning rate.\n",
        "        gamma: Discount factor.\n",
        "        epsilon_start: Starting exploration rate.\n",
        "        epsilon_end: Minimum exploration rate.\n",
        "        decay_rate: Epsilon decay rate.\n",
        "\n",
        "    Returns:\n",
        "        Q: The learned Q-value function.\n",
        "        policy: The optimal policy derived from Q.\n",
        "    \"\"\"\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    epsilon = epsilon_start # Set starting epsilon value\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-Greedy randomized action selection\n",
        "            if random.randint(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            # Take the action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value using Bellman equation\n",
        "            best_next_action = np.argmax(Q[next_state])  # Max Q value for the next state\n",
        "            Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "        # Epsilon will decay each episode\n",
        "        epsilon = max(epsilon_end, epsilon * decay_rate)\n",
        "\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "qGlT9mul5lOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning with decay\n",
        "Q, optimal_policy = q_learning2(env)\n",
        "print(f\"Optimal Policy: \\n{optimal_policy.reshape((4, 4))}\")\n",
        "print(f\"Optimal Q-Values:\\n{Q}\")\n",
        "\n",
        "# Optimal policy\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi6U3rxD6ws2",
        "outputId": "1ad629bc-d1fa-4a30-f175-88dd2989a250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: \n",
            "[[1 3 1 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 3 3 0]]\n",
            "Optimal Q-Values:\n",
            "[[0.02355972 0.02463036 0.02369449 0.02210632]\n",
            " [0.01155378 0.01298456 0.0141779  0.02550483]\n",
            " [0.02957713 0.03424103 0.02948144 0.02157563]\n",
            " [0.0103027  0.01371659 0.00852264 0.02102668]\n",
            " [0.03517407 0.02219203 0.01634041 0.0208715 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.04738669 0.03074347 0.04532827 0.00517788]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.02387975 0.04730862 0.0359639  0.05395988]\n",
            " [0.04324442 0.09454847 0.06209223 0.0278745 ]\n",
            " [0.13926184 0.07814057 0.11960327 0.0310159 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.03254301 0.0625088  0.06063448 0.14145995]\n",
            " [0.11595006 0.34819098 0.35298562 0.41853957]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Average Reward: 0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing rewards of Q-Learning with and without decaying epsilon"
      ],
      "metadata": {
        "id": "uwUi65GM62aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning without decay\n",
        "Q_basic, policy_basic = q_learning(env)\n",
        "\n",
        "# Q-Learning with decay\n",
        "Q_decay, policy_decay = q_learning2(env)\n",
        "\n",
        "# Evaluate both policies\n",
        "average_reward_basic = evaluate_policy(env, policy_basic)\n",
        "average_reward_decay = evaluate_policy(env, policy_decay)\n",
        "\n",
        "print(\"Average Reward with Basic Q-Learning:\", average_reward_basic)\n",
        "print(\"Average Reward with Q-Learning (Decaying Epsilon):\", average_reward_decay)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw0Jn-GH5rnC",
        "outputId": "374b29b5-4ce8-4267-b1ff-7e4032383ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Reward with Basic Q-Learning: 0.19\n",
            "Average Reward with Q-Learning (Decaying Epsilon): 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have tested with decay_rate values ranging from 0.95 to 0.5 and the results vary each execution. Sometimes the function without decaying epsilon performs better, but, after numerous executions, it is clear that the decaying epsilon method offers increased performance and provides a greater reward majority of the time."
      ],
      "metadata": {
        "id": "jJE0KnSE7CNh"
      }
    }
  ]
}