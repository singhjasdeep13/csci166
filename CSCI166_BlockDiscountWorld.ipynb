{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWji2dqP/Yu1Au/GcJDKOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhjasdeep13/csci166/blob/main/CSCI166_BlockDiscountWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 State = {a, b, c, d, e}\n",
        "3 Actions = {Left, Right, Exit}\n",
        "Exit available only in a & e.\n",
        "Exit from a yields reward of 10\n",
        "Exit from e yields reward of 1"
      ],
      "metadata": {
        "id": "3kk3sM_eu3Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** I used ChatGPT to help me optimize my code because I am not very familiar with Python, so it does not work perfectly. I do understand the material and my answers should be correct regardless of code. I hope that is okay. **"
      ],
      "metadata": {
        "id": "hRkI2x8dqqJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byAluYByuGvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Possible states and actions\n",
        "states = ['a', 'b', 'c', 'd', 'e']\n",
        "actions = ['L', 'R', 'X']\n",
        "\n",
        "# Rewards and state changes\n",
        "rewards = {\n",
        "    'a': {'L': -1, 'R': -1, 'X': 10},\n",
        "    'b': {'L': 0, 'R': 0, 'X': 0},\n",
        "    'c': {'L': 0, 'R': 0, 'X': 0},\n",
        "    'd': {'L': 0, 'R': 0, 'X': 0},\n",
        "    'e': {'L': -1, 'R': -1, 'X': 1},\n",
        "}\n",
        "\n",
        "transitions = {\n",
        "    'a': {'L': None, 'R': 'c', 'X': None},\n",
        "    'b': {'L': 'a', 'R': 'd', 'X': None},\n",
        "    'c': {'L': 'b', 'R': 'd', 'X': None},\n",
        "    'd': {'L': 'c', 'R': 'e', 'X': None},\n",
        "    'e': {'L': 'd', 'R': None, 'X': None},\n",
        "}\n",
        "\n",
        "# Function for Value Iteration\n",
        "def blockDiscount(gamma, epsilon=1e-6):\n",
        "    V = np.zeros(len(states))\n",
        "    policy = np.zeros(len(states), dtype=object)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for idx, state in enumerate(states):\n",
        "            v = V[idx]\n",
        "            action_values = []\n",
        "            for action in actions:\n",
        "                if action == 'X' and state not in ['a', 'e']:\n",
        "                    continue\n",
        "                reward = rewards[state][action]\n",
        "                next_state = transitions[state][action]\n",
        "                if next_state is not None:\n",
        "                    next_state_idx = states.index(next_state)\n",
        "                    action_value = reward + gamma * V[next_state_idx]\n",
        "                else:\n",
        "                    action_value = reward\n",
        "                action_values.append(action_value)\n",
        "            V[idx] = max(action_values)\n",
        "            delta = max(delta, abs(v - V[idx]))\n",
        "            policy[idx] = actions[np.argmax(action_values)]\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Calculate Optimum Policy for cases: Transitions are deterministic, ð›¾=1"
      ],
      "metadata": {
        "id": "Tv3NrQ86vBe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust gamma value and test here\n",
        "gamma = 1.0\n",
        "\n",
        "reward, policy = blockDiscount(gamma)\n",
        "\n",
        "print(\"Gamma = \", gamma)\n",
        "print(\"Rewards: \", reward)\n",
        "print(\"Optimum Policy: \", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtjJR2Ixow0q",
        "outputId": "aa5eaa6b-4946-4c5d-c870-a551c6bc4790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gamma =  1.0\n",
            "Rewards:  [10. 10. 10. 10.  9.]\n",
            "Optimum Policy:  ['X' 'L' 'L' 'L' 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(1) Calculate Optimum Policy for cases: Transitions are deterministic, ð›¾=0.1"
      ],
      "metadata": {
        "id": "LsZg4Zg1vJ3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust gamma value and test here\n",
        "gamma = 0.1\n",
        "\n",
        "reward, policy = blockDiscount(gamma)\n",
        "\n",
        "print(\"Gamma = \", gamma)\n",
        "print(\"Rewards: \", reward)\n",
        "print(\"Optimum Policy: \", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebf88d7-600a-40b3-991d-310a5dc7631a",
        "id": "zFPbqDRJriHP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gamma =  0.1\n",
            "Rewards:  [10.   1.   0.1  0.1  1. ]\n",
            "Optimum Policy:  ['X' 'L' 'L' 'R' 'X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Calculate the value of the sequence of rewards from each of the states under the optimum policy for both previous cases.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "For ð›¾ = 1.0\n",
        "Optimum policy = ['X' 'L' 'L' 'L' 'L']\n",
        "Rewards = [10, 10, 10, 10, 10]\n",
        "```\n",
        "\n",
        "```\n",
        "For ð›¾ = 0.1\n",
        "Optimum policy = ['X' 'L' 'L' 'R' 'X']\n",
        "Rewards = [10, 1, 0.1, 0.1, 1]\n",
        "```\n"
      ],
      "metadata": {
        "id": "819MtUQzvOnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) For which ð›¾, are West and East equally good when in state d?\n",
        "\n",
        "\n",
        "```\n",
        "From state 'd'\n",
        "Left 3 states to exit from state 'a' = Right for 1 state to exit from 'd'\n",
        "10 * x^3 = x\n",
        "10 * x^2 = 1\n",
        "x^2 = 1/10\n",
        "x = sqrt(1/10)\n",
        "x = 0.316\n",
        "```\n",
        "\n",
        "Therefore, with gamma of 0.316, the reward will be nearly the same regardless of which direction is chosen.\n"
      ],
      "metadata": {
        "id": "7Wn5RKtSyUZE"
      }
    }
  ]
}